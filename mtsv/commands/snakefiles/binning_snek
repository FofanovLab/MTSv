import os
from mtsv.utils import (
    script_path, get_item_from_artifact)

shell.prefix("set -euo pipefail;")


INDEX_PATHS = get_item_from_artifact(
    config['database_config'], "fm-index-paths")

INDEX_MAP = {
    os.path.splitext(
        os.path.basename(p))[0]: p for p in INDEX_PATHS} 



MODE_DICT = {'fast': {'seed_size': 17, 'min_seeds': 5, 'seed_gap': 2},
             'efficient': {'seed_size': 14, 'min_seeds': 4, 'seed_gap': 2},
             'sensitive': {'seed_size': 11, 'min_seeds': 3, 'seed_gap': 1}}

BIN_MODE = MODE_DICT[config.get('binning_mode', "efficient") ]
for key, mode_value in BIN_MODE.items():
    if key not in config:
        # only overwrite with mode value if value is not
        # provided in config
        config[key] = mode_value


rule unaligned_queries:
    """
    Get queries with no hits.
    """
    input: 
        clp = "binning/{sample}_merged.clp",
        queries = "queries/{sample}.fasta"
    output: "unaligned_queries/{sample}_unaligned.fasta",
    log: "logs/unaligned_queries_{sample}.log"
    threads: 1
    message:
        """
        Finding unaligned queries.
        Writing unaligned queries to {output}.
        Logging to {log}.
        Using {threads} thread(s).
        """
    run:
        from mtsv.scripts.unaligned_queries import get_unaligned_queries
        from mtsv.utils import config_logging
        config_logging(log[0], "INFO")
        logging.info("Getting unaligned queries")
        get_unaligned_queries(input.queries, input.clp, output[0])
        logging.info("Finished getting unaligned queries")

            
rule binning:
    """
    Alignment-based metagenomic binning
    """
    input:
        queries = "queries/{sample}.fasta",
        index = lambda wc: INDEX_MAP[wc.index]
    output: temp("binning/sample_{sample}_index_{index}.bn")
    log: "logs/binning_sample_{sample}_index_{index}.log"
    threads: 128
    params:
        edits=config.get('edits', 3),
        max_hits=config.get('max_hits', 20000),
        seed_size=config['seed_size'],
        min_seeds=config['min_seeds'],
        seed_gap=config['seed_gap']
    message:
        """
        Executing Binner on queries in {input.queries}.
        Using index {input.index}
        Writing files to directory {output}.
        Logging to {log}.
        Using {threads} thread(s).
        Parameters: edits={params.edits}, seed_size={params.seed_size}
        min_seeds={params.min_seeds}, seed_gap={params.seed_gap}
        """
    shell:
        """
        if [[ -s {input.queries} ]]
            then
                mtsv-binner \
                --index {input.index} --fasta {input.queries} \
                --results {output} --threads {threads} \
                --max-hits {params.max_hits} \
                --edits {params.edits} --seed-size {params.seed_size} \
                --min-seeds {params.min_seeds} \
                --seed-gap {params.seed_gap} >> {log} 2>&1
            else
                touch {output}
                echo "WARNING: Empty query file" >> {log}
        fi
        """


rule collapse:
    """
    Combine binning results from different FM-indices.
    """
    input:
        expand(
        "binning/sample_{{sample}}_index_{index}.bn",
        index=list(INDEX_MAP.keys()))
    output: "binning/{sample}_merged.clp"
    log: "logs/{sample}_merged.log"
    threads: 1
    message:
        """
        Merging binning output into {output}.
        Logging to {log}.
        Using {threads} thread(s).
        """
    shell:
        """
        mtsv-collapse {input} -o {output} >> {log} 2>&1
        """



# rule binning_report:
#     input:
#         os.path.join(config['binning_outpath'],
#             'query_stats.json')
#     output:
#         rep=REPORT
#     params:
#         header = SAMPLE_NAMES,
#         merge_file = config['merge_file']
#     message:
#         """
#         Running binning report.
#         Writing report to {output.rep}
#         Snakemake scheduler assuming {threads} thread(s)
#         """
#     # resources:
#     #     mem_mb=lambda wildcards, input, attempt: max(1,
#     #         attempt * int(
#     #             os.path.getsize(input.clps) * 0.000001 +
#     #             os.path.getsize(input.fasta) * 0.000002))
#     run:
#         from snakemake.utils import report

#         summary_data = json.loads(open(input[0], 'r').read())

#         table = make_table(
#             [params.header, summary_data['total_queries_by_sample'],
#             summary_data['total_unique_queries_by_sample'],
#             summary_data['total_hits_by_sample'],
#             summary_data['total_unique_hits_by_sample'],
#             summary_data['total_unaligned_queries_by_sample'],
#             summary_data['total_unique_unaligned_queries_by_sample']],
#             ["Sample", "Total Queries", "Total Unique Queries",
#             "Total Hits", "Total Unique Hits", "Unaligned Queries",
#             "Unique Unaligned Queries"])

#         file_str = "\n".join(UNALIGNED_QUERIES)
#         total_hits = summary_data['total_hits']
#         total_unique_hits = summary_data['total_unique_hits']
#         total_queries = summary_data['total_queries']
#         total_unique_queries = summary_data['total_unique_queries']
#         total_unaligned_queries = summary_data['total_unaligned_queries']
#         total_unique_unaligned_queries = summary_data[
#             'total_unique_unaligned_queries']

#         track_file_params('merge_file', config['merge_file'], config)

#         report("""
#         Binning Report
#         ================================
#         Hits are reported in:\n
#         **{params.merge_file}**\n
#         There were **{total_hits}** total hits
#         and **{total_unique_hits}** unique hits out
#         of **{total_queries}** total queries and
#         **{total_unique_queries}** unique queries.\n
#         There were **{total_unaligned_queries}** total
#         queries and **{total_unique_unaligned_queries}**
#         unique queries with no hits.\n
#         Unaligned query sequences are reported in:\n
#         {file_str}.\n
#         Query Stats:\n
#         {table}
#         """, output['rep'])
