from mtsv.utils import (
    config_logging, script_path, get_item_from_artifact)

TAXDUMP = get_item_from_artifact(
    config['database_config'], "taxdump-path")


rule extract:
    """
    Extract queries that hit a given a taxid.
    """
    input:
        queries = "queries/{sample}.fasta",
        clp = "binning/{sample}_merged.clp"
    output: 
        fasta = "extracted_queries/{sample}_taxid_{taxa}.fasta"
    log: "logs/extract_queries_{sample}_taxid_{taxa}.log"
    params:
        max_taxa_per_query = config.get("max_taxa_per_query", 50),
        taxdump = TAXDUMP,
        chunksize = config.get("chunksize", 100000)
    threads: 1
    message:
        """
        Extracting query sequences from {input.queries}
        for taxids: {wildcards.taxa}
        that were hits in {input.clp}.
        Writing to {output}.
        Logging to {log}.
        Using {threads} thread(s)
        """
    run:
        from mtsv.scripts.summary import parse_clp_file_in_chunks_for_extract
        config_logging(log[0], "INFO")
        logging.info("Extracting hits for taxid: {0}".format(wildcards.taxa))
        parse_clp_file_in_chunks_for_extract(
            params.taxdump, input.clp, input.queries,
            wildcards.taxa, output.fasta,
            params.chunksize, params.max_taxa_per_query)
        logging.info(
            "Finished extracting hits, written to {}".format(output.fasta))






# rule extract_report:
#     input:
#         fasta = FASTA_OUTPUT,
#         fastq = FASTQ_OUTPUT
#     output:
#         REPORT
#     # resources:
#     #     mem_mb=lambda wildcards, input, attempt: max(1, attempt * int(
#     #         os.path.getsize(input[0]) * 0.000001 +
#     #         os.path.getsize(input[1]) * 0.000001))
#     resources:
#         mem_mb = 200
#     message:
#         """
#         Generating extract report from {input.fasta} and {input.fastq}
#         Writing to {output}
#         Snakemake scheduler assuming {threads} threads and
#         {resources.mem_mb} Mb of memory. """

#     params:
#         path = config['extract_path'],
#         source = config['input_hits'],
#         taxa = config['taxids']
#     run:
#         from snakemake.utils import report
#         total_fasta_queries = [
#             len(list(SeqIO.parse(fasta, "fasta"))) for fasta in input.fasta]
#         total_fastq_queries = [
#             len(list(SeqIO.parse(fastq, "fastq"))) for fastq in input.fastq]
#         fasta_files = [os.path.basename(out) for out in input.fasta]
#         fastq_files = [os.path.basename(out) for out in input.fastq]
#         _files = fasta_files + fastq_files
#         total_queries = total_fasta_queries + total_fastq_queries
#         data = [_files, total_queries]
#         data = make_table(data, ["File", "No. Queries"])
#         # data = "\n".join(["{0:<26} {1:<11}".format(f, q) for f, q in zip(
#         #     _files, total_queries)])
#         report(
#             """ 
#             Extract Report
#             ============================
#             Hits from **{params.source}** for taxids **{params.taxa}**
#             were extracted to fasta and fastq files at:\n
#             **{params.path}**\n
#             Number of queries per file:\n
#             {data}

#             """, output[0])
